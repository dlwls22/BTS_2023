{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlwks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "from PIL import Image, ImageFile, ImageOps\n",
    "from torch.utils import data as data_utils\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Histogram_Equalization_pil(image):\n",
    "    r, g, b = image.split()\n",
    "    r_eq = ImageOps.equalize(r)\n",
    "    g_eq = ImageOps.equalize(g)\n",
    "    b_eq = ImageOps.equalize(b)\n",
    "    histogram_image = Image.merge(\"RGB\", (r_eq, g_eq, b_eq))\n",
    "    return np.array(histogram_image)\n",
    "\n",
    "def load_image_pil(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image_pil(path):\n",
    "    image = load_image_pil(path)\n",
    "    image = Histogram_Equalization_pil(image)\n",
    "    return Image.fromarray(image)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(data_path)\n",
    "        self.images = self.load_images()\n",
    "\n",
    "    def load_images(self):\n",
    "        images = []\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(self.data_path, class_name)\n",
    "            for image_name in os.listdir(class_path):\n",
    "                image_path = os.path.join(class_path, image_name)\n",
    "                images.append((image_path, class_name))\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path, class_name = self.images[index]\n",
    "        image = load_and_preprocess_image_pil(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.classes.index(class_name)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.RandomRotation(1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))  \n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_data_path = r'C:\\Users\\dlwks\\OneDrive\\바탕 화면\\VSCode\\BTS_2023\\train'\n",
    "train_dataset = CustomDataset(train_data_path, transform=train_transform)\n",
    "\n",
    "test_data_path = r'C:\\Users\\dlwks\\OneDrive\\바탕 화면\\VSCode\\BTS_2023\\test'\n",
    "test_dataset = CustomDataset(test_data_path, transform=test_transform) \n",
    "\n",
    "image, label = train_dataset[0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "## StartifiedShuufleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=val_size, random_state=42)\n",
    "train_indices, val_indices = next(sss.split(train_dataset, [label for _, label in train_dataset.images]))\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 32, sampler = train_sampler)\n",
    "val_loader = DataLoader(train_dataset, batch_size = 32, sampler = val_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.bencmark = True\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(nn.functional.softplus(x))\n",
    "\n",
    "class GIGAJINI(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GIGAJINI, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            Mish(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size = 3, stride =1, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            Mish(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            Mish(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            Mish(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "\n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            Mish(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride= 2)\n",
    "        )\n",
    "\n",
    "        self.layer6 = torch.nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            Mish(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "\n",
    "        self.layer7 = torch.nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            Mish(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "        \n",
    "        self.fc =torch.nn.Sequential(\n",
    "            nn.Linear(512 * 2 * 2, 2, bias = True),            \n",
    "            Mish()\n",
    "        )        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GIGAJINI().to(device)\n",
    "learning_rate = 1e-4\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "class_weight = torch.tensor([0.1, 0.9])\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = class_weight).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs, learning_rate, patience):\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    epochs_without_importvement = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "\n",
    "        for X, Y in train_loader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "        avg_loss /= len(train_loader)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader) \n",
    "\n",
    "        print(f'Epoch : {epoch + 1}, Train Loss : {avg_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model.state_dict().copy()\n",
    "            print('Model Saved')\n",
    "            epochs_without_importvement = 0\n",
    "\n",
    "        else:\n",
    "            epochs_without_importvement += 1\n",
    "\n",
    "        if epochs_without_importvement >= patience:\n",
    "            print(f'Early stopping: No improvement in validation loss for {patience} epochs.')\n",
    "            break\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in dataloader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Train Loss : 0.2358, Validation Loss: 0.6448\n",
      "Model Saved\n",
      "Epoch : 2, Train Loss : 0.0776, Validation Loss: 0.5156\n",
      "Model Saved\n",
      "Epoch : 3, Train Loss : 0.0673, Validation Loss: 0.2755\n",
      "Model Saved\n",
      "Epoch : 4, Train Loss : 0.0563, Validation Loss: 0.1021\n",
      "Model Saved\n",
      "Epoch : 5, Train Loss : 0.0613, Validation Loss: 0.0547\n",
      "Model Saved\n",
      "Epoch : 6, Train Loss : 0.0525, Validation Loss: 0.0638\n",
      "Epoch : 7, Train Loss : 0.0441, Validation Loss: 0.0563\n",
      "Epoch : 8, Train Loss : 0.0426, Validation Loss: 0.0559\n",
      "Epoch : 9, Train Loss : 0.0388, Validation Loss: 0.0387\n",
      "Model Saved\n",
      "Epoch : 10, Train Loss : 0.0366, Validation Loss: 0.0502\n",
      "Epoch : 11, Train Loss : 0.0317, Validation Loss: 0.0395\n",
      "Epoch : 12, Train Loss : 0.0360, Validation Loss: 0.0402\n",
      "Epoch : 13, Train Loss : 0.0283, Validation Loss: 0.0457\n",
      "Epoch : 14, Train Loss : 0.0186, Validation Loss: 0.0236\n",
      "Model Saved\n",
      "Epoch : 15, Train Loss : 0.0214, Validation Loss: 0.0299\n",
      "Epoch : 16, Train Loss : 0.0155, Validation Loss: 0.0150\n",
      "Model Saved\n",
      "Epoch : 17, Train Loss : 0.0133, Validation Loss: 0.0245\n",
      "Epoch : 18, Train Loss : 0.0130, Validation Loss: 0.0197\n",
      "Epoch : 19, Train Loss : 0.0146, Validation Loss: 0.0143\n",
      "Model Saved\n",
      "Epoch : 20, Train Loss : 0.0106, Validation Loss: 0.0217\n",
      "Epoch : 21, Train Loss : 0.0099, Validation Loss: 0.0114\n",
      "Model Saved\n",
      "Epoch : 22, Train Loss : 0.0096, Validation Loss: 0.0111\n",
      "Model Saved\n",
      "Epoch : 23, Train Loss : 0.0099, Validation Loss: 0.0221\n",
      "Epoch : 24, Train Loss : 0.0086, Validation Loss: 0.0109\n",
      "Model Saved\n",
      "Epoch : 25, Train Loss : 0.0082, Validation Loss: 0.0110\n",
      "Epoch : 26, Train Loss : 0.0085, Validation Loss: 0.0278\n",
      "Epoch : 27, Train Loss : 0.0093, Validation Loss: 0.0175\n",
      "Epoch : 28, Train Loss : 0.0092, Validation Loss: 0.0123\n",
      "Epoch : 29, Train Loss : 0.0090, Validation Loss: 0.0160\n",
      "Early stopping: No improvement in validation loss for 5 epochs.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 256, 256] doesn't match the broadcast shape [3, 256, 256]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dlwks\\OneDrive\\바탕 화면\\VSCode\\BTS_2023\\CNN_최종.ipynb 셀 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dlwks/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/VSCode/BTS_2023/CNN_%EC%B5%9C%EC%A2%85.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m all_labels \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dlwks/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/VSCode/BTS_2023/CNN_%EC%B5%9C%EC%A2%85.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dlwks/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/VSCode/BTS_2023/CNN_%EC%B5%9C%EC%A2%85.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mfor\u001b[39;00m X, Y \u001b[39min\u001b[39;00m test_loader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dlwks/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/VSCode/BTS_2023/CNN_%EC%B5%9C%EC%A2%85.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dlwks/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/VSCode/BTS_2023/CNN_%EC%B5%9C%EC%A2%85.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         Y \u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\dlwks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\dlwks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\dlwks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\dlwks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\dlwks\\OneDrive\\바탕 화면\\VSCode\\BTS_2023\\CNN_최종.ipynb 셀 9\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dlwks/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/VSCode/BTS_2023/CNN_%EC%B5%9C%EC%A2%85.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m image \u001b[39m=\u001b[39m load_and_preprocess_image_pil(image_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dlwks/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/VSCode/BTS_2023/CNN_%EC%B5%9C%EC%A2%85.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dlwks/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/VSCode/BTS_2023/CNN_%EC%B5%9C%EC%A2%85.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dlwks/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/VSCode/BTS_2023/CNN_%EC%B5%9C%EC%A2%85.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mreturn\u001b[39;00m image, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses\u001b[39m.\u001b[39mindex(class_name)\n",
      "File \u001b[1;32mc:\\Users\\dlwks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\dlwks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dlwks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Users\\dlwks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    361\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Users\\dlwks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:928\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[39mif\u001b[39;00m std\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    927\u001b[0m     std \u001b[39m=\u001b[39m std\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m--> 928\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49msub_(mean)\u001b[39m.\u001b[39mdiv_(std)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: output with shape [1, 256, 256] doesn't match the broadcast shape [3, 256, 256]"
     ]
    }
   ],
   "source": [
    "patience = 5\n",
    "best_model = train(model, train_loader, val_loader, epochs, learning_rate, patience)\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, Y in test_loader:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        output = model(X)\n",
    "        test_loss += criterion(output, Y).item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(Y.view_as(pred)).sum().item()\n",
    "\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(Y.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = correct / len(test_loader.dataset)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# F1 score 계산\n",
    "f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(f\"F1 Score (Micro): {f1_micro:.8f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.8f}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted:.8f}\")\n",
    "\n",
    "# Precision 계산\n",
    "precision_micro = precision_score(all_labels, all_preds, average='micro')\n",
    "precision_macro = precision_score(all_labels, all_preds, average='macro')\n",
    "precision_weighted = precision_score(all_labels, all_preds, average='weighted')\n",
    "print(f\"Precision (Micro): {precision_micro:.8f}\")\n",
    "print(f\"Precision (Macro): {precision_macro:.8f}\")\n",
    "print(f\"Precision (Weighted): {precision_weighted:.8f}\")\n",
    "\n",
    "# Recall 계산\n",
    "recall_micro = recall_score(all_labels, all_preds, average='micro')\n",
    "recall_macro = recall_score(all_labels, all_preds, average='macro')\n",
    "recall_weighted = recall_score(all_labels, all_preds, average='weighted')\n",
    "print(f\"Recall (Micro): {recall_micro:.8f}\")\n",
    "print(f\"Recall (Macro): {recall_macro:.8f}\")\n",
    "print(f\"Recall (Weighted): {recall_weighted:.8f}\")\n",
    "\n",
    "# Confusion Matrix 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# 분류 리포트 출력\n",
    "class_names = [str(num) for num in torch.arange(2).tolist()]\n",
    "classification_rep = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "print('Classification Report:')\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'CNN.pth'\n",
    "\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mport numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 정규 분포의 평균과 표준 편차 설정\n",
    "# mean = 8\n",
    "# std_dev = -2\n",
    "\n",
    "# # x 값 범위 설정\n",
    "# x = np.linspace(mean - 3 * std_dev, mean + 3 * std_dev, 100)\n",
    "# # 정규 분포 곡선을 생성합니다.\n",
    "# y = (1 / (std_dev * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean) / std_dev)**2)\n",
    "\n",
    "# # 그래프 그리기\n",
    "# plt.plot(x, y, label=f\"평균={mean}, 표준편차={std_dev}\")\n",
    "# plt.title('정규 분포 그래프')\n",
    "# plt.xlabel('X 축')\n",
    "# plt.ylabel('Y 축')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# # 그래프 표시\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
