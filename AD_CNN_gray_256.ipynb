{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as data\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFile, ImageOps\n",
    "from torch.utils import data as data_utils\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Histogram_Equalization_pil(image):\n",
    "    r, g, b = image.split()\n",
    "    r_eq = ImageOps.equalize(r)\n",
    "    g_eq = ImageOps.equalize(g)\n",
    "    b_eq = ImageOps.equalize(b)\n",
    "    histogram_image = Image.merge(\"RGB\", (r_eq, g_eq, b_eq))\n",
    "    return np.array(histogram_image)\n",
    "\n",
    "def load_image_pil(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image_pil(path):\n",
    "    image = load_image_pil(path)\n",
    "    image = Histogram_Equalization_pil(image)\n",
    "    return Image.fromarray(image)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(data_path)\n",
    "        self.images = self.load_images()\n",
    "\n",
    "    def load_images(self):\n",
    "        images = []\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(self.data_path, class_name)\n",
    "            for image_name in os.listdir(class_path):\n",
    "                image_path = os.path.join(class_path, image_name)\n",
    "                images.append((image_path, class_name))\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path, class_name = self.images[index]\n",
    "        image = load_and_preprocess_image_pil(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.classes.index(class_name)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.RandomRotation(1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))  \n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_data_path = r'C:\\Users\\dlwks\\OneDrive\\바탕 화면\\VSCode\\BTS_2023\\remove_dataset\\train'\n",
    "train_dataset = CustomDataset(train_data_path, transform=train_transform)\n",
    "\n",
    "test_data_path = r'C:\\Users\\dlwks\\OneDrive\\바탕 화면\\VSCode\\BTS_2023\\remove_dataset\\test'\n",
    "test_dataset = CustomDataset(test_data_path, transform=test_transform) \n",
    "\n",
    "image, label = train_dataset[0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "## StartifiedShuufleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=val_size, random_state=42)\n",
    "train_indices, val_indices = next(sss.split(train_dataset, [label for _, label in train_dataset.images]))\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 32, sampler = train_sampler)\n",
    "val_loader = DataLoader(train_dataset, batch_size = 32, sampler = val_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.bencmark = True\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(nn.functional.softplus(x))\n",
    "\n",
    "class GIGAJINI(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GIGAJINI, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            Mish(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size = 3, stride =1, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            Mish(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            Mish(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            Mish(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "\n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            Mish(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride= 2)\n",
    "        )\n",
    "\n",
    "        self.layer6 = torch.nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            Mish(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "\n",
    "        self.layer7 = torch.nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            Mish(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "        \n",
    "        self.fc =torch.nn.Sequential(\n",
    "            nn.Linear(512 * 2 * 2, 2, bias = True),            \n",
    "            Mish()\n",
    "        )        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GIGAJINI().to(device)\n",
    "learning_rate = 1e-4\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "class_weight = torch.tensor([0.1, 0.9])\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = class_weight).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs, learning_rate, patience):\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    epochs_without_importvement = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "\n",
    "        for X, Y in train_loader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "        avg_loss /= len(train_loader)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader) \n",
    "\n",
    "        print(f'Epoch : {epoch + 1}, Train Loss : {avg_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model.state_dict().copy()\n",
    "            print('Model Saved')\n",
    "            epochs_without_importvement = 0\n",
    "\n",
    "        else:\n",
    "            epochs_without_importvement += 1\n",
    "\n",
    "        if epochs_without_importvement >= patience:\n",
    "            print(f'Early stopping: No improvement in validation loss for {patience} epochs.')\n",
    "            break\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in dataloader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Train Loss : 0.8248, Validation Loss: 0.6893\n",
      "Model Saved\n",
      "Epoch : 2, Train Loss : 0.5569, Validation Loss: 0.6880\n",
      "Model Saved\n",
      "Epoch : 3, Train Loss : 0.7223, Validation Loss: 0.6897\n",
      "Epoch : 4, Train Loss : 0.6172, Validation Loss: 0.6923\n",
      "Epoch : 5, Train Loss : 0.4446, Validation Loss: 0.6962\n",
      "Epoch : 6, Train Loss : 0.4842, Validation Loss: 0.6999\n",
      "Epoch : 7, Train Loss : 0.5040, Validation Loss: 0.7049\n",
      "Early stopping: No improvement in validation loss for 5 epochs.\n",
      "Test Loss: 0.1407, Accuracy: 60.00%\n",
      "F1 Score (Micro): 0.60000000\n",
      "F1 Score (Macro): 0.37500000\n",
      "F1 Score (Weighted): 0.45000000\n",
      "Precision (Micro): 0.60000000\n",
      "Precision (Macro): 0.30000000\n",
      "Precision (Weighted): 0.36000000\n",
      "Recall (Micro): 0.60000000\n",
      "Recall (Macro): 0.50000000\n",
      "Recall (Weighted): 0.60000000\n",
      "Confusion Matrix:\n",
      "[[3 0]\n",
      " [2 0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75         3\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.30      0.50      0.37         5\n",
      "weighted avg       0.36      0.60      0.45         5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlwks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dlwks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dlwks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dlwks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dlwks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "patience = 5\n",
    "best_model = train(model, train_loader, val_loader, epochs, learning_rate, patience)\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, Y in test_loader:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        output = model(X)\n",
    "        test_loss += criterion(output, Y).item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(Y.view_as(pred)).sum().item()\n",
    "\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(Y.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = correct / len(test_loader.dataset)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# F1 score 계산\n",
    "f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(f\"F1 Score (Micro): {f1_micro:.8f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.8f}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted:.8f}\")\n",
    "\n",
    "# Precision 계산\n",
    "precision_micro = precision_score(all_labels, all_preds, average='micro')\n",
    "precision_macro = precision_score(all_labels, all_preds, average='macro')\n",
    "precision_weighted = precision_score(all_labels, all_preds, average='weighted')\n",
    "print(f\"Precision (Micro): {precision_micro:.8f}\")\n",
    "print(f\"Precision (Macro): {precision_macro:.8f}\")\n",
    "print(f\"Precision (Weighted): {precision_weighted:.8f}\")\n",
    "\n",
    "# Recall 계산\n",
    "recall_micro = recall_score(all_labels, all_preds, average='micro')\n",
    "recall_macro = recall_score(all_labels, all_preds, average='macro')\n",
    "recall_weighted = recall_score(all_labels, all_preds, average='weighted')\n",
    "print(f\"Recall (Micro): {recall_micro:.8f}\")\n",
    "print(f\"Recall (Macro): {recall_macro:.8f}\")\n",
    "print(f\"Recall (Weighted): {recall_weighted:.8f}\")\n",
    "\n",
    "# Confusion Matrix 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# 분류 리포트 출력\n",
    "class_names = [str(num) for num in torch.arange(2).tolist()]\n",
    "classification_rep = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "print('Classification Report:')\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1로 예측했는데 0인 것들\n",
    "# class_1_predictions = []\n",
    "# for i, (X, Y) in enumerate(test_dataset):\n",
    "#     X = X.to(device)\n",
    "#     output = model(X.unsqueeze(0))  # 배치 차원을 추가하여 예측합니다.\n",
    "#     pred = output.argmax(dim=1, keepdim=True)\n",
    "#     if pred.item() == 1:\n",
    "#         class_1_predictions.append((X.cpu(), Y, pred.item()))\n",
    "\n",
    "# num_to_display = min(len(class_1_predictions), 10)\n",
    "\n",
    "# for i in range(num_to_display):\n",
    "#     image, true_label, predicted_label = class_1_predictions[i]\n",
    "#     image = (image.squeeze().numpy() + 1) / 2  # 이미지를 원래 범위로 변환합니다.\n",
    "#     plt.imshow(image, cmap='gray')\n",
    "#     plt.title(f\"True Label: {true_label}, Predicted Label: {predicted_label}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1_predictions = []\n",
    "for i, (X, Y) in enumerate(test_dataset):\n",
    "    X = X.to(device)\n",
    "    output = model(X.unsqueeze(0))  # 배치 차원을 추가하여 예측합니다.\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    if pred.item() == 1 and Y == 1: \n",
    "        class_1_predictions.append((X.cpu(), Y, pred.item()))\n",
    "\n",
    "num_to_display = min(len(class_1_predictions), 10)\n",
    "\n",
    "for i in range(num_to_display):\n",
    "    image, true_label, predicted_label = class_1_predictions[i]\n",
    "    image = (image.squeeze().numpy() + 1) / 2  # 이미지를 원래 범위로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'AD_CNN_gray_256.pth'\n",
    "\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
